{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b26f57-f978-4d4f-9412-bf6cbc781e98",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "## Introduction\n",
    "The goal of topic modeling is to find various latent topics that are present in the corpus.  Each document in the corpus will be made up of one or more topics.  One approach to topic modeling is know as Latent Dirichlet Allocation (LDA).  In LDA documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow Dirichlet probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens.  LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "## Input\n",
    "1. Document Term Matrix with additional stop words removed from the eda notebook\n",
    "2. The number of topics to find in the reviews\n",
    "\n",
    "## Output\n",
    "1. Once the topic modeling technique is applied, you need to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model.\n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 UFO Software, LLC\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d10201-f9e7-4e7b-b60a-7eb6f9522e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import tqdm as notebook_tqdm\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import matutils, models\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import warnings\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.util import minibatch\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8d4f68-888a-40e5-9036-3919b3895ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the directory structure\n",
    "# use a temp directory to keep intermediate results\n",
    "parent_dir = '/run/user/1000/gvfs/smb-share:server=titan.local,share=data_sets/strains'\n",
    "temp_dir = parent_dir+'/temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58409e9a-594f-42e6-8ec2-8adb1af1a15d",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1\n",
    "Use all the text from the initial data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3a284-3134-4461-9bd5-f3e5f9d92662",
   "metadata": {},
   "source": [
    "## Document Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e440b20-a246-4903-b882-8043697399cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_all</th>\n",
       "      <th>a_couch_locker</th>\n",
       "      <th>a_life_saver</th>\n",
       "      <th>a_roller_coaster</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaqaaaaaaaa</th>\n",
       "      <th>...</th>\n",
       "      <th>zzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>ıf</th>\n",
       "      <th>łēčtrpart</th>\n",
       "      <th>ʻohana</th>\n",
       "      <th>ʻono</th>\n",
       "      <th>δthc</th>\n",
       "      <th>⅛th</th>\n",
       "      <th>⅛thweight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strain</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24k-gold</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-kings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3x-crazy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501st-og</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoda-og</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yogi-diesel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yumboldt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeus-og</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 57675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             _all  a_couch_locker  a_life_saver  a_roller_coaster  aa  aaa  \\\n",
       "strain                                                                       \n",
       "1024            0               0             0                 0   0    0   \n",
       "24k-gold        0               0             0                 0   0    0   \n",
       "3-kings         0               0             0                 0   0    0   \n",
       "3x-crazy        0               0             0                 0   0    0   \n",
       "501st-og        0               0             0                 0   0    0   \n",
       "...           ...             ...           ...               ...  ..  ...   \n",
       "yoda-og         0               0             0                 0   0    0   \n",
       "yogi-diesel     0               0             0                 0   0    0   \n",
       "yumboldt        0               0             0                 0   0    0   \n",
       "yummy           0               0             0                 0   0    0   \n",
       "zeus-og         0               0             0                 0   0    0   \n",
       "\n",
       "             aaaa  aaaaa  aaaaaaaaaaaaaa  aaaaaaaaaaaaaqaaaaaaaa  ...  \\\n",
       "strain                                                            ...   \n",
       "1024            0      0               0                       0  ...   \n",
       "24k-gold        0      0               0                       0  ...   \n",
       "3-kings         0      0               0                       0  ...   \n",
       "3x-crazy        0      0               0                       0  ...   \n",
       "501st-og        0      0               0                       0  ...   \n",
       "...           ...    ...             ...                     ...  ...   \n",
       "yoda-og         0      0               0                       0  ...   \n",
       "yogi-diesel     0      0               0                       0  ...   \n",
       "yumboldt        0      0               0                       0  ...   \n",
       "yummy           0      0               0                       0  ...   \n",
       "zeus-og         0      0               0                       0  ...   \n",
       "\n",
       "             zzzzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "strain                                                    \n",
       "1024                       0                          0   \n",
       "24k-gold                   0                          0   \n",
       "3-kings                    0                          0   \n",
       "3x-crazy                   0                          0   \n",
       "501st-og                   0                          0   \n",
       "...                      ...                        ...   \n",
       "yoda-og                    0                          0   \n",
       "yogi-diesel                0                          0   \n",
       "yumboldt                   0                          0   \n",
       "yummy                      0                          0   \n",
       "zeus-og                    0                          0   \n",
       "\n",
       "             zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "strain                                                                                  \n",
       "1024                                                         0                          \n",
       "24k-gold                                                     0                          \n",
       "3-kings                                                      0                          \n",
       "3x-crazy                                                     0                          \n",
       "501st-og                                                     0                          \n",
       "...                                                        ...                          \n",
       "yoda-og                                                      0                          \n",
       "yogi-diesel                                                  0                          \n",
       "yumboldt                                                     0                          \n",
       "yummy                                                        0                          \n",
       "zeus-og                                                      0                          \n",
       "\n",
       "             ıf  łēčtrpart  ʻohana  ʻono  δthc  ⅛th  ⅛thweight  \n",
       "strain                                                          \n",
       "1024          0          0       0     0     0    0          0  \n",
       "24k-gold      0          0       0     0     0    0          0  \n",
       "3-kings       0          0       0     0     0    0          0  \n",
       "3x-crazy      0          0       0     0     0    0          0  \n",
       "501st-og      0          0       0     0     0    0          0  \n",
       "...          ..        ...     ...   ...   ...  ...        ...  \n",
       "yoda-og       0          0       0     0     0    0          0  \n",
       "yogi-diesel   0          0       0     0     0    0          0  \n",
       "yumboldt      0          0       0     0     0    0          0  \n",
       "yummy         0          0       0     0     0    0          0  \n",
       "zeus-og       0          0       0     0     0    0          0  \n",
       "\n",
       "[1056 rows x 57675 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the document term matrix\n",
    "data = pd.read_parquet(temp_dir+'/dtm_stop.parquet')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372811a-e9c8-4a0a-aa67-731025dc6efb",
   "metadata": {},
   "source": [
    "## Term Document Matrix (TDM)\n",
    "the transpose of the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f6982a-d797-4f06-80c1-c01099dcc0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>strain</th>\n",
       "      <th>1024</th>\n",
       "      <th>24k-gold</th>\n",
       "      <th>3-kings</th>\n",
       "      <th>3x-crazy</th>\n",
       "      <th>501st-og</th>\n",
       "      <th>5th-element</th>\n",
       "      <th>707-headband</th>\n",
       "      <th>8-ball-kush</th>\n",
       "      <th>818-og</th>\n",
       "      <th>91-krypt</th>\n",
       "      <th>...</th>\n",
       "      <th>wookies</th>\n",
       "      <th>wsu</th>\n",
       "      <th>xxx-og</th>\n",
       "      <th>y-griega</th>\n",
       "      <th>yeti-og</th>\n",
       "      <th>yoda-og</th>\n",
       "      <th>yogi-diesel</th>\n",
       "      <th>yumboldt</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zeus-og</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>_all</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_couch_locker</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_life_saver</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_roller_coaster</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1056 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "strain            1024  24k-gold  3-kings  3x-crazy  501st-og  5th-element  \\\n",
       "_all                 0         0        0         0         0            0   \n",
       "a_couch_locker       0         0        0         0         0            0   \n",
       "a_life_saver         0         0        0         0         0            0   \n",
       "a_roller_coaster     0         0        0         0         0            0   \n",
       "aa                   0         0        0         0         0            0   \n",
       "\n",
       "strain            707-headband  8-ball-kush  818-og  91-krypt  ...  wookies  \\\n",
       "_all                         0            0       0         0  ...        0   \n",
       "a_couch_locker               0            0       0         0  ...        0   \n",
       "a_life_saver                 0            0       0         0  ...        0   \n",
       "a_roller_coaster             0            0       0         0  ...        0   \n",
       "aa                           1            0       0         0  ...        0   \n",
       "\n",
       "strain            wsu  xxx-og  y-griega  yeti-og  yoda-og  yogi-diesel  \\\n",
       "_all                0       0         0        0        0            0   \n",
       "a_couch_locker      0       0         0        1        0            0   \n",
       "a_life_saver        0       0         0        0        0            0   \n",
       "a_roller_coaster    0       0         0        0        0            0   \n",
       "aa                  0       0         0        0        0            0   \n",
       "\n",
       "strain            yumboldt  yummy  zeus-og  \n",
       "_all                     0      0        0  \n",
       "a_couch_locker           0      0        0  \n",
       "a_life_saver             0      0        0  \n",
       "a_roller_coaster         0      0        0  \n",
       "aa                       0      0        0  \n",
       "\n",
       "[5 rows x 1056 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose the dtm to create a term document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449e81a5-6426-4a6c-b00b-94a31128cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a17b926-722d-465d-9342-fdf6d3a403b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(temp_dir+\"/cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9903476-284b-4c8b-a9c5-7cbfac75c494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"sativa\" + 0.006*\"strong\" + 0.006*\"effect\" + 0.006*\"happy\" + 0.006*\"time\" + 0.006*\"head\" + 0.005*\"amazing\" + 0.005*\"sweet\" + 0.005*\"flavor\" + 0.005*\"anxiety\"'),\n",
       " (1,\n",
       "  '0.009*\"pain\" + 0.008*\"indica\" + 0.007*\"effect\" + 0.007*\"sleep\" + 0.006*\"help\" + 0.006*\"anxiety\" + 0.006*\"strong\" + 0.006*\"relax\" + 0.005*\"relaxed\" + 0.005*\"head\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32935266-1c19-47dd-b80d-ec356339aa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"pain\" + 0.007*\"effect\" + 0.006*\"anxiety\" + 0.006*\"sativa\" + 0.006*\"time\" + 0.006*\"strong\" + 0.006*\"head\" + 0.006*\"help\" + 0.005*\"happy\" + 0.005*\"amazing\"'),\n",
       " (1,\n",
       "  '0.007*\"sweet\" + 0.007*\"indica\" + 0.007*\"effect\" + 0.007*\"strong\" + 0.006*\"flavor\" + 0.006*\"pain\" + 0.005*\"relax\" + 0.005*\"sleep\" + 0.005*\"relaxed\" + 0.005*\"time\"'),\n",
       " (2,\n",
       "  '0.006*\"strong\" + 0.006*\"indica\" + 0.006*\"effect\" + 0.005*\"sleep\" + 0.005*\"happy\" + 0.005*\"head\" + 0.005*\"sweet\" + 0.005*\"amazing\" + 0.005*\"pain\" + 0.004*\"flavor\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d45d23-fa93-4533-91fc-b67a6a325c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"sweet\" + 0.005*\"effect\" + 0.005*\"amazing\" + 0.005*\"flavor\" + 0.005*\"happy\" + 0.004*\"head\" + 0.004*\"smooth\" + 0.004*\"indica\" + 0.004*\"strong\" + 0.004*\"pretty\"'),\n",
       " (1,\n",
       "  '0.007*\"effect\" + 0.007*\"strong\" + 0.006*\"pain\" + 0.006*\"time\" + 0.006*\"head\" + 0.006*\"sativa\" + 0.005*\"happy\" + 0.005*\"indica\" + 0.005*\"anxiety\" + 0.005*\"amazing\"'),\n",
       " (2,\n",
       "  '0.007*\"sativa\" + 0.007*\"strong\" + 0.006*\"head\" + 0.006*\"time\" + 0.006*\"happy\" + 0.006*\"effect\" + 0.005*\"flavor\" + 0.005*\"sweet\" + 0.005*\"amazing\" + 0.004*\"anxiety\"'),\n",
       " (3,\n",
       "  '0.010*\"pain\" + 0.007*\"anxiety\" + 0.007*\"effect\" + 0.007*\"help\" + 0.006*\"sleep\" + 0.006*\"indica\" + 0.006*\"relax\" + 0.005*\"cbd\" + 0.005*\"relaxed\" + 0.005*\"time\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ed30fb-77a7-4c10-973a-c9e07d939e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tri_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it a good even head and body high good for str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you can change the name give it no name call i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I be skeptical_about this strain after try thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this strain be always a favorite the top_favor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have m and this strain be suggest for I to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>superdank I finally_find my medicine I can_nt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>this strain provide a nice head high where you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>this strain be excellent for relieve my migrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>really like this one nice body high great for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>probably call zeus og because with the cloud y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tri_review\n",
       "0     it a good even head and body high good for str...\n",
       "1     you can change the name give it no name call i...\n",
       "2     I be skeptical_about this strain after try thr...\n",
       "3     this strain be always a favorite the top_favor...\n",
       "4     I have m and this strain be suggest for I to h...\n",
       "...                                                 ...\n",
       "1051  superdank I finally_find my medicine I can_nt ...\n",
       "1052  this strain provide a nice head high where you...\n",
       "1053  this strain be excellent for relieve my migrai...\n",
       "1054  really like this one nice body high great for ...\n",
       "1055  probably call zeus og because with the cloud y...\n",
       "\n",
       "[1056 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_parquet(temp_dir+'/tri_grams.parquet')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447f086-39bc-4674-b325-5f7132927f86",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2\n",
    "Use only nouns, proper nouns and adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22dc95ca-ebd5-4163-9cdc-ded1720e3469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.4.0/en_core_web_trf-3.4.0-py3-none-any.whl (460.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.3/460.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from en-core-web-trf==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from en-core-web-trf==3.4.0) (1.1.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (65.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (8.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.23.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (4.21.3)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (0.8.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (1.12.1.post200)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (0.12.1)\n",
      "Requirement already satisfied: filelock in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (2022.9.13)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/moogedelic/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "custom_stop_words = set(['effect', 'strong', 'day', 'hit', 'amazing', 'favorite', 'little', 'one'])\n",
    "cls = spacy.util.get_lang_class('en')\n",
    "cls.Defaults.stop_words = custom_stop_words\n",
    "!python -m spacy download en_core_web_trf\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1200e9fd-61b1-4a21-93bd-d60fd0cb5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    nouns_adj = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN' or token.pos_ == 'ADJ':\n",
    "            if not token.is_stop:\n",
    "                nouns_adj.append(token.text)\n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc77d09f-700c-42c0-917a-71565ad2f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tri_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good even head body high good stress nice high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name name schnauzerganjkosher tangie k gold we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skeptical_about strain kings amazed stress top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strain top_favorite in_fact potency strain dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strain muscle_spasm first second_time before_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>superdank medicine active imagination depressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>strain nice head high thought forefront high n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>strain excellent migraine chance strain enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>nice body high great every_day light head high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>zeus og cloud able_to thunder hard perfect pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tri_review\n",
       "0     good even head body high good stress nice high...\n",
       "1     name name schnauzerganjkosher tangie k gold we...\n",
       "2     skeptical_about strain kings amazed stress top...\n",
       "3     strain top_favorite in_fact potency strain dep...\n",
       "4     strain muscle_spasm first second_time before_b...\n",
       "...                                                 ...\n",
       "1051  superdank medicine active imagination depressi...\n",
       "1052  strain nice head high thought forefront high n...\n",
       "1053  strain excellent migraine chance strain enough...\n",
       "1054  nice body high great every_day light head high...\n",
       "1055  zeus og cloud able_to thunder hard perfect pre...\n",
       "\n",
       "[1056 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns and adjectives\n",
    "nouns_adj_file = temp_dir+'/data_nouns_adj.parquet'\n",
    "if not exists(nouns_adj_file):\n",
    "    data_nouns_adj = pd.DataFrame(data_clean.tri_review.apply(lambda x: nouns_adj(x)))\n",
    "    data_nouns_adj.to_parquet(nouns_adj_file)\n",
    "else:\n",
    "    data_nouns_adj = pd.read_parquet(nouns_adj_file)\n",
    "    \n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04859351-8015-45ed-a0ca-a93cf4229a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_couch_locker</th>\n",
       "      <th>a_life_saver</th>\n",
       "      <th>a_roller_coaster</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aampf</th>\n",
       "      <th>aarch</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abad</th>\n",
       "      <th>...</th>\n",
       "      <th>zzzzzzgreat</th>\n",
       "      <th>zzzzzznighty</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "      <th>zzzzzzzzi</th>\n",
       "      <th>zzzzzzzzzzz</th>\n",
       "      <th>łēčtrpart</th>\n",
       "      <th>ʻohana</th>\n",
       "      <th>δthc</th>\n",
       "      <th>⅛th</th>\n",
       "      <th>⅛thweight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 42704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a_couch_locker  a_life_saver  a_roller_coaster  aa  aaa  aampf  aarch  \\\n",
       "0                  0             0                 0   0    0      0      0   \n",
       "1                  0             0                 0   0    0      0      0   \n",
       "2                  0             0                 0   0    0      0      0   \n",
       "3                  0             0                 0   0    0      0      0   \n",
       "4                  0             0                 0   0    0      0      0   \n",
       "...              ...           ...               ...  ..  ...    ...    ...   \n",
       "1051               0             0                 0   0    0      0      0   \n",
       "1052               0             0                 0   0    0      0      0   \n",
       "1053               0             0                 0   0    0      0      0   \n",
       "1054               0             0                 0   0    0      0      0   \n",
       "1055               0             0                 0   0    0      0      0   \n",
       "\n",
       "      aaron  ab  abad  ...  zzzzzzgreat  zzzzzznighty  zzzzzzzz  zzzzzzzzi  \\\n",
       "0         0   0     0  ...            0             0         0          0   \n",
       "1         0   0     0  ...            0             0         0          0   \n",
       "2         0   0     0  ...            0             0         0          0   \n",
       "3         0   0     0  ...            0             0         0          0   \n",
       "4         0   0     0  ...            0             0         0          0   \n",
       "...     ...  ..   ...  ...          ...           ...       ...        ...   \n",
       "1051      0   0     0  ...            0             0         0          0   \n",
       "1052      0   0     0  ...            0             0         0          0   \n",
       "1053      0   0     0  ...            0             0         0          0   \n",
       "1054      0   0     0  ...            0             0         0          0   \n",
       "1055      0   0     0  ...            0             0         0          0   \n",
       "\n",
       "      zzzzzzzzzzz  łēčtrpart  ʻohana  δthc  ⅛th  ⅛thweight  \n",
       "0               0          0       0     0    0          0  \n",
       "1               0          0       0     0    0          0  \n",
       "2               0          0       0     0    0          0  \n",
       "3               0          0       0     0    0          0  \n",
       "4               0          0       0     0    0          0  \n",
       "...           ...        ...     ...   ...  ...        ...  \n",
       "1051            0          0       0     0    0          0  \n",
       "1052            0          0       0     0    0          0  \n",
       "1053            0          0       0     0    0          0  \n",
       "1054            0          0       0     0    0          0  \n",
       "1055            0          0       0     0    0          0  \n",
       "\n",
       "[1056 rows x 42704 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.tri_review)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc93754-3adc-43bb-951f-b1cc35e71bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf8e494e-e6b6-47c7-a866-97423bdd3a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"pain\" + 0.009*\"indica\" + 0.008*\"time\" + 0.008*\"head\" + 0.007*\"sweet\" + 0.007*\"flavor\" + 0.007*\"relaxed\" + 0.007*\"anxiety\" + 0.007*\"happy\" + 0.006*\"smooth\"'),\n",
       " (1,\n",
       "  '0.012*\"sativa\" + 0.009*\"anxiety\" + 0.008*\"pain\" + 0.008*\"happy\" + 0.008*\"time\" + 0.007*\"head\" + 0.006*\"cbd\" + 0.006*\"sweet\" + 0.005*\"uplifting\" + 0.005*\"flavor\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c7efbe3-7039-49ad-a1d9-a14b56156a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"happy\" + 0.008*\"sativa\" + 0.007*\"time\" + 0.006*\"flavor\" + 0.006*\"head\" + 0.006*\"sweet\" + 0.005*\"smooth\" + 0.005*\"hybrid\" + 0.005*\"lemon\" + 0.004*\"uplifting\"'),\n",
       " (1,\n",
       "  '0.012*\"sativa\" + 0.009*\"happy\" + 0.008*\"time\" + 0.008*\"head\" + 0.007*\"anxiety\" + 0.007*\"sweet\" + 0.007*\"flavor\" + 0.006*\"uplifting\" + 0.006*\"smooth\" + 0.005*\"pain\"'),\n",
       " (2,\n",
       "  '0.013*\"pain\" + 0.011*\"indica\" + 0.009*\"anxiety\" + 0.008*\"relaxed\" + 0.008*\"time\" + 0.007*\"head\" + 0.007*\"sweet\" + 0.006*\"flavor\" + 0.006*\"sleep\" + 0.006*\"smooth\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f81b8ce0-4177-4f95-9184-d546ce995e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"sativa\" + 0.008*\"time\" + 0.008*\"happy\" + 0.007*\"sweet\" + 0.007*\"head\" + 0.006*\"pain\" + 0.006*\"anxiety\" + 0.006*\"flavor\" + 0.005*\"uplifting\" + 0.005*\"smooth\"'),\n",
       " (1,\n",
       "  '0.011*\"sativa\" + 0.009*\"happy\" + 0.008*\"time\" + 0.008*\"head\" + 0.007*\"flavor\" + 0.007*\"anxiety\" + 0.006*\"sweet\" + 0.006*\"smooth\" + 0.005*\"pain\" + 0.005*\"stuff\"'),\n",
       " (2,\n",
       "  '0.012*\"pain\" + 0.010*\"anxiety\" + 0.008*\"head\" + 0.008*\"time\" + 0.007*\"cbd\" + 0.007*\"happy\" + 0.006*\"sweet\" + 0.006*\"relaxed\" + 0.006*\"flavor\" + 0.005*\"smooth\"'),\n",
       " (3,\n",
       "  '0.013*\"indica\" + 0.011*\"pain\" + 0.009*\"relaxed\" + 0.008*\"sweet\" + 0.007*\"sleep\" + 0.007*\"time\" + 0.007*\"head\" + 0.007*\"flavor\" + 0.007*\"anxiety\" + 0.007*\"heavy\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03ade451-8ec3-4541-942f-5a04ee0f3fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"acdc\" + 0.003*\"orange_crush\" + 0.002*\"mk_ultra\" + 0.002*\"cbd\" + 0.002*\"sour_tsunami\" + 0.002*\"harletsu\" + 0.001*\"panama_red\" + 0.001*\"remedy\" + 0.001*\"blueberry_headband\" + 0.001*\"voodoo\"'),\n",
       " (1,\n",
       "  '0.009*\"sativa\" + 0.009*\"happy\" + 0.008*\"time\" + 0.008*\"head\" + 0.007*\"sweet\" + 0.007*\"flavor\" + 0.006*\"anxiety\" + 0.006*\"pain\" + 0.006*\"relaxed\" + 0.006*\"smooth\"'),\n",
       " (2,\n",
       "  '0.013*\"pain\" + 0.009*\"anxiety\" + 0.009*\"indica\" + 0.008*\"time\" + 0.008*\"head\" + 0.007*\"relaxed\" + 0.007*\"flavor\" + 0.007*\"sweet\" + 0.006*\"happy\" + 0.006*\"smooth\"'),\n",
       " (3,\n",
       "  '0.020*\"sativa\" + 0.007*\"haze\" + 0.006*\"sweet\" + 0.006*\"energetic\" + 0.006*\"uplifting\" + 0.005*\"mango\" + 0.005*\"pineapple\" + 0.005*\"strawberry\" + 0.004*\"happy\" + 0.004*\"cherry_pie\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final 4 topic model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf3486f1-9307-4c83-bb85-c61dcd754e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"indica\" + 0.010*\"pain\" + 0.008*\"sweet\" + 0.008*\"relaxed\" + 0.008*\"time\" + 0.007*\"head\" + 0.007*\"flavor\" + 0.007*\"anxiety\" + 0.006*\"smooth\" + 0.006*\"heavy\"'),\n",
       " (1,\n",
       "  '0.012*\"sativa\" + 0.009*\"happy\" + 0.009*\"anxiety\" + 0.009*\"time\" + 0.008*\"head\" + 0.008*\"pain\" + 0.006*\"flavor\" + 0.006*\"uplifting\" + 0.006*\"sweet\" + 0.005*\"smooth\"'),\n",
       " (2,\n",
       "  '0.006*\"gelato\" + 0.004*\"bubble_gum\" + 0.003*\"bubblegum\" + 0.003*\"chernobyl\" + 0.002*\"orange_crush\" + 0.002*\"blue_diesel\" + 0.002*\"key_lime_pie\" + 0.002*\"pink_kush\" + 0.002*\"nebula\" + 0.002*\"sweet_tooth\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final 3 topic model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6cc31a6-7948-4c2e-b129-b29431b88423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"pain\" + 0.011*\"indica\" + 0.009*\"anxiety\" + 0.008*\"relaxed\" + 0.007*\"sleep\" + 0.007*\"time\" + 0.007*\"head\" + 0.006*\"heavy\" + 0.006*\"flavor\" + 0.006*\"relaxing\"'),\n",
       " (1,\n",
       "  '0.011*\"sativa\" + 0.009*\"happy\" + 0.009*\"time\" + 0.008*\"head\" + 0.008*\"sweet\" + 0.007*\"flavor\" + 0.006*\"anxiety\" + 0.006*\"smooth\" + 0.006*\"hybrid\" + 0.005*\"uplifting\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Findal 2 topic model\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3815f-387d-4b66-ab61-d0e3b7a256f5",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document\n",
    "The 2 topic model makes the most sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "618a2906-cc11-4d5d-a4a6-e4b9d2c4cc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"pain\" + 0.012*\"indica\" + 0.010*\"anxiety\" + 0.008*\"sleep\" + 0.007*\"relaxed\" + 0.007*\"time\" + 0.007*\"cbd\" + 0.006*\"head\" + 0.006*\"heavy\" + 0.006*\"relaxing\"'),\n",
       " (1,\n",
       "  '0.009*\"sativa\" + 0.009*\"happy\" + 0.008*\"time\" + 0.008*\"head\" + 0.008*\"sweet\" + 0.007*\"flavor\" + 0.007*\"anxiety\" + 0.006*\"smooth\" + 0.006*\"pain\" + 0.006*\"relaxed\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final 2 topic model\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5976c0-39c8-4c63-8a58-f7469c42f3f5",
   "metadata": {},
   "source": [
    "## The Topics Discussed in the Reviews are Indica and Sativa\n",
    "Hybrid constituted about half of the reviews but it does not have its own topic.  Hybrid is a cross between indica and sativa so it difficult to distinguish hybrid from the two other species since it holds properties of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35123c4-fe12-4540-a486-4dc3659a7d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
